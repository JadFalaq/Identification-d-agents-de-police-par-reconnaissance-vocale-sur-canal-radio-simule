import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import time
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from config import OUTPUT_ROOT

print("\n" + "="*70)
print("ğŸ“ SECTION 6 : ENTRAÃNEMENT DU MODÃˆLE")
print("="*70)

# ========================================
# 1. CHARGEMENT DATASET
# ========================================

print("\nğŸ“‚ Chargement du dataset...")
dataset_path = OUTPUT_ROOT / 'features' / 'police_radio_dataset.csv'

if not dataset_path.exists():
    print(f"âŒ Dataset introuvable: {dataset_path}")
    exit(1)

dataset_df = pd.read_csv(dataset_path)
print(f"âœ… ChargÃ©: {len(dataset_df)} Ã©chantillons")

# ========================================
# 2. PRÃ‰PARATION
# ========================================

print("\nğŸ“Š PrÃ©paration des donnÃ©es...")
X = dataset_df.drop('speaker_id', axis=1).values
y = dataset_df['speaker_id'].values

print(f"   Dataset shape: {X.shape}")
print(f"   Agents uniques: {len(np.unique(y))}")

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"\nğŸ·ï¸  Encodage des labels:")
print(f"   Exemple: {y[0]} â†’ {y_encoded[0]}")
print(f"   Classes: {len(label_encoder.classes_)}")

# ========================================
# 3. SPLIT (70/15/15)
# ========================================

print(f"\nâœ‚ï¸  Split du dataset...")

X_temp, X_test, y_temp, y_test = train_test_split(
    X, y_encoded, 
    test_size=0.15, 
    random_state=42, 
    stratify=y_encoded
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, 
    test_size=0.176,
    random_state=42, 
    stratify=y_temp
)

print(f"   Train: {len(X_train):5d} ({len(X_train)/len(X)*100:.1f}%)")
print(f"   Val:   {len(X_val):5d} ({len(X_val)/len(X)*100:.1f}%)")
print(f"   Test:  {len(X_test):5d} ({len(X_test)/len(X)*100:.1f}%)")

num_agents = len(label_encoder.classes_)
print(f"\nâš–ï¸  Balance:")
print(f"   Train: {len(X_train) / num_agents:.1f} samples/agent")
print(f"   Val:   {len(X_val) / num_agents:.1f} samples/agent")
print(f"   Test:  {len(X_test) / num_agents:.1f} samples/agent")

# ========================================
# 4. NORMALISATION
# ========================================

print(f"\nğŸ“ Normalisation StandardScaler...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print(f"   Mean avant: {X_train.mean():.3f}")
print(f"   Mean aprÃ¨s: {X_train_scaled.mean():.3f}")
print(f"   Std aprÃ¨s:  {X_train_scaled.std():.3f}")

# ========================================
# 5. ENTRAÃNEMENT SVM
# ========================================

print(f"\n" + "="*70)
print("â³ ENTRAÃNEMENT SVM (kernel RBF)...")
print("="*70)
print(f"â±ï¸  Estimation: 3-5 minutes pour {num_agents} agents")
print(f"ğŸ“Š Dataset: {len(X_train):,} Ã— {X.shape[1]} features")
print(f"ğŸ¯ Classes: {num_agents}\n")

start_time = time.time()

model = SVC(
    kernel='rbf',
    C=33,
    gamma=0.0045,
    random_state=42,
    verbose=False
)

model.fit(X_train_scaled, y_train)

training_time = time.time() - start_time
print(f"\nâœ… EntraÃ®nement: {training_time:.1f}s ({training_time/60:.1f} min)")

# ========================================
# 6. PRÃ‰DICTIONS
# ========================================

print(f"\nğŸ”® PrÃ©dictions...")
y_train_pred = model.predict(X_train_scaled)
y_val_pred = model.predict(X_val_scaled)
y_test_pred = model.predict(X_test_scaled)

# ========================================
# 7. MÃ‰TRIQUES
# ========================================

print(f"\n" + "="*70)
print("ğŸ“Š RÃ‰SULTATS DE PERFORMANCE")
print("="*70)

train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

print(f"\nğŸ¯ ACCURACY:")
print(f"   Train: {train_acc:.4f} ({train_acc*100:.2f}%)")
print(f"   Val:   {val_acc:.4f} ({val_acc*100:.2f}%)")
print(f"   Test:  {test_acc:.4f} ({test_acc*100:.2f}%)")

print(f"\nğŸ’¡ INTERPRÃ‰TATION:")
if test_acc >= 0.90:
    print("   â­ EXCELLENT!")
elif test_acc >= 0.85:
    print("   âœ… TRÃˆS BON!")
elif test_acc >= 0.80:
    print("   âœ… BON!")
else:
    print("   âš ï¸  MOYEN")

overfitting = train_acc - test_acc
print(f"\nğŸ” Ã‰cart Train-Test: {overfitting:.4f} ({overfitting*100:.2f}%)")
if overfitting < 0.05:
    print("   âœ… Pas d'overfitting")
elif overfitting < 0.10:
    print("   âš ï¸  LÃ©ger overfitting")
else:
    print("   âŒ Overfitting important")

# ========================================
# 8. MATRICE DE CONFUSION
# ========================================

print(f"\nğŸ“Š Matrice de confusion...")

cm = confusion_matrix(y_test, y_test_pred)
cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

fig, axes = plt.subplots(1, 2, figsize=(18, 8))

sns.heatmap(cm, cmap='Blues', cbar=True, square=True, 
            xticklabels=False, yticklabels=False, ax=axes[0],
            annot=False, fmt='d')
axes[0].set_title(f'Matrice - {num_agents} Agents\nTest: {test_acc:.2%}', 
                  fontsize=14, fontweight='bold')
axes[0].set_xlabel('PrÃ©dit', fontsize=12)
axes[0].set_ylabel('RÃ©el', fontsize=12)

diagonal_accuracy = np.diag(cm).sum() / cm.sum()
axes[0].text(0.5, -0.1, f'Diagonale: {diagonal_accuracy:.2%}', 
             transform=axes[0].transAxes, ha='center', fontsize=11)

error_rates = 100 - np.diag(cm_percent)
axes[1].hist(error_rates, bins=20, color='coral', edgecolor='darkred', alpha=0.7)
axes[1].axvline(error_rates.mean(), color='red', linestyle='--', linewidth=2,
                label=f'Moy: {error_rates.mean():.1f}%')
axes[1].set_title('Distribution Erreurs', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Taux erreur (%)', fontsize=12)
axes[1].set_ylabel('Agents', fontsize=12)
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()

perf_path = OUTPUT_ROOT / 'models' / 'model_performance_50agents.png'
plt.savefig(perf_path, dpi=150, bbox_inches='tight')
print(f"   âœ… {perf_path}")
plt.show()
plt.close()

# ========================================
# 9. RAPPORT DÃ‰TAILLÃ‰
# ========================================

print(f"\nğŸ“‹ RAPPORT:")
report = classification_report(y_test, y_test_pred, 
                               target_names=label_encoder.classes_,
                               output_dict=True, zero_division=0)

report_df = pd.DataFrame(report).T
report_df = report_df[report_df.index.str.startswith('id')]
report_df = report_df.sort_values('f1-score', ascending=False)

print("\nğŸ† TOP 5:")
print(report_df.head(5)[['precision', 'recall', 'f1-score', 'support']].to_string())

print("\nâš ï¸  WORST 5:")
print(report_df.tail(5)[['precision', 'recall', 'f1-score', 'support']].to_string())

# ========================================
# 10. SAUVEGARDE
# ========================================

print(f"\nğŸ’¾ Sauvegarde...")

models_dir = OUTPUT_ROOT / 'models'
models_dir.mkdir(exist_ok=True)

model_path = models_dir / 'svm_police_radio_model.pkl'
scaler_path = models_dir / 'scaler.pkl'
encoder_path = models_dir / 'label_encoder.pkl'

joblib.dump(model, model_path)
joblib.dump(scaler, scaler_path)
joblib.dump(label_encoder, encoder_path)

print(f"   âœ… {model_path.name}")
print(f"   âœ… {scaler_path.name}")
print(f"   âœ… {encoder_path.name}")

print("\n" + "="*70)
print("ğŸ‰ SECTION 6 TERMINÃ‰E!")
print("="*70)

print(f"\nğŸ“Š RÃ‰SUMÃ‰:")
print(f"   Agents: {len(label_encoder.classes_)}")
print(f"   Features: {X.shape[1]}")
print(f"   Test Accuracy: {test_acc:.2%}")
print(f"   Temps: {training_time:.1f}s")
print(f"   Fichiers:")
print(f"    - svm_police_radio_model.pkl")
print(f"    - scaler.pkl")
print(f"    - label_encoder.pkl")
print(f"    - model_performance_50agents.png")
print("="*70 + "\n")
